
# Resnet 

ğŸ› ResNet enables the construction of deeper neural networks, with more than a hundred layers, which was previously impossible due to the vanishing gradient problem.

ğŸ› I [@yota](https://github.com/yotaAI) am Implementing the model from Paper ğŸ“„ : https://arxiv.org/pdf/1512.03385


## ğŸ“ Knowledge

The model is having seperate `Residual` connection block, for this with increasing number of layers the accuracy of the model will also increase. 


âœï¸ I am training the model with Imagenet Dataset.

âœï¸ For Learning Purpose we are initializing the model with random weights.

âœï¸ I am using one scheduler to devide the learning rate by `10` when the test loss come to a stable position.
 

## ğŸ—ƒï¸ Dataset

ğŸ—ï¸ Training on Imagenet Dataset with 1000 classes. For dataset [Click](https://www.image-net.org/challenges/LSVRC/index.php).

ğŸ—ï¸As described in paper[Page 4] The model is first trained on dataset of `Scale=256`  then the pretrained model is again trained on `Scale=384` with `Learning Rate = 10^-3`.


## ğŸ¤– Training VGG

ğŸ·ï¸ Now run the `train.py` and Boom!ğŸ¤¯
```bash
python3 train.py
```

ğŸ·ï¸ Model will be saved in `./resnet/`.




## ğŸ¥·ğŸ» Ninja Tech

âš¡ï¸ Machine Learning âš¡ï¸ Deep Learning âš¡ï¸ CNN âš¡ï¸


